{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cbf6a73-b8c0-466a-85c1-1c317eaaaabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Bronze layer**  - Ingesting data to databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e331b5d-71c6-49d7-b310-a581bd29bdef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Defining Spark Session\n",
    "spark=SparkSession.builder.appName(\"JobETL_Bronze\").getOrCreate()\n",
    "\n",
    "API_KEY = \"630f7f2b9c335376c24528a0c6997245ea046c46929b4ead4211322c87bf9064\"\n",
    "\n",
    "roles = [\"Data Engineer\", \"Data Analyst\", \"Python Developer\", \"ETL Developer\", \"Spark Engineer\"]\n",
    "location = \"India\"\n",
    "all_jobs=[]\n",
    "\n",
    "#running a loop over the job roles\n",
    "for role in roles:\n",
    "    params={\n",
    "        \"engine\":\"google_jobs\",\n",
    "        \"q\":role,\n",
    "        \"location\":location,\n",
    "        \"api_key\":API_KEY\n",
    "    }\n",
    "    res = requests.get(\"https://serpapi.com/search.json\", params=params)\n",
    "\n",
    "    print(\"reading live data from google jobs api\")\n",
    "\n",
    "    jobs = res.json().get(\"jobs_results\", [])\n",
    "\n",
    "    for job in jobs:\n",
    "        job[\"search_role\"] = role\n",
    "    all_jobs.extend(jobs)\n",
    "\n",
    "#bronze_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(job) for job in all_jobs]))\n",
    "\n",
    "bronze_df = spark.createDataFrame(all_jobs)\n",
    "bronze_df.write.format(\"delta\").option(\"mergeSchema\",'true').mode(\"overwrite\").saveAsTable(\"bronze_jobs_raw\")\n",
    "\n",
    "print(\"data written to bronze layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce37650b-cd3e-404c-94eb-924f9da735f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Silver Layer** - cleaning and structuring data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cecc300-d720-4ef9-a833-763b0b50c7dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark=SparkSession.builder.appName(\"JobETL_Silver\").getOrCreate()\n",
    "\n",
    "#reading bronze layer\n",
    "print(\"reading from broze layer to silver layer\")\n",
    "df =spark.read.table(\"bronze_jobs_raw\")\n",
    "\n",
    "#cleaning and structuring\n",
    "silver_df= df.selectExpr(\n",
    "    \"title\",\n",
    "    \"company_name\",\n",
    "    \"location\",\n",
    "    \"description\",\n",
    "    \"job_id\",\n",
    "    \"detected_extensions.posted_at as posted_at\",\n",
    "    \"search_role\"\n",
    ").dropna(subset=[\"title\",\"company_name\",\"location\"])\n",
    " \n",
    "#display(silver_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f449921-b91e-4c0a-b394-14548417e439",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753102629241}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#trimming company name column and make it upper \n",
    "silver_df=silver_df.withColumn(\"company_name\",trim(upper(col(\"company_name\"))))\n",
    "#dropping na in posted at column\n",
    "silver_df=silver_df.dropna(subset=[\"posted_at\"])\n",
    "\n",
    "#writing the data to silver table\n",
    "silver_df.write.format(\"delta\").option(\"mergeSchema\",'true').mode(\"overwrite\").saveAsTable(\"silver_jobs_raw\")\n",
    "\n",
    "print(\"Data written to silver layer\")\n",
    "#display(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de02c7bb-c61e-4ac7-869a-7bc2c4f0eb1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Golden layer** - generate KPI table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdaeb4b1-2c3f-4cc8-b3f8-f7400e566a0e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Gold layer method 1"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark=SparkSession.builder.appName(\"JobETL_Gold\").getOrCreate()\n",
    "print(\"reading from silver layer to gold layer\")\n",
    "df =spark.read.table(\"silver_jobs_raw\")\n",
    "\n",
    "# KPI 1 top companies\n",
    "top_companies=df.groupBy(\"company_name\").count().orderBy(col(\"count\").desc())\n",
    "#display(top_companies)\n",
    "\n",
    "\n",
    "# KPI 2jobs by city\n",
    "\n",
    "job_by_city=df.groupBy(\"location\").agg(count(\"*\").alias('job_count')).orderBy(col('job_count').desc())\n",
    "#display(job_by_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9fef21c-215f-4642-822e-1cef76aefef2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "GOld layer method 2"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark=SparkSession.builder.appName(\"JobETL_Gold\").getOrCreate()\n",
    "\n",
    "df =spark.read.table(\"silver_jobs_raw\")\n",
    "df.write.mode(\"append\").saveAsTable(\"job_aggregator_table\")\n",
    "\n",
    "print(\"data successfully written to taBLE -job_aggregator_table \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d81cbdf8-66b7-4143-b2d0-32e3ec326df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"full ETL job aggregator project is completed and live!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f233bf-8c93-45e1-a963-fd5c4ccde28f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "describe history job_aggregator_table"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7523262051159085,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "PROJECT",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
