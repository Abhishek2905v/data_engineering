{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cbf6a73-b8c0-466a-85c1-1c317eaaaabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Bronze layer**  - Ingesting data to databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e331b5d-71c6-49d7-b310-a581bd29bdef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Defining Spark Session\n",
    "spark=SparkSession.builder.appName(\"JobETL_Bronze\").getOrCreate()\n",
    "\n",
    "API_KEY = \"630f7f2b9c335376c24528a0c6997245ea046c46929b4ead4211322c87bf9064\"\n",
    "\n",
    "roles = [\"Data Engineer\", \"Data Analyst\", \"Python Developer\", \"ETL Developer\", \"Spark Engineer\"]\n",
    "location = \"India\"\n",
    "all_jobs=[]\n",
    "\n",
    "#running a loop over the job roles\n",
    "for role in roles:\n",
    "    params={\n",
    "        \"engine\":\"google_jobs\",\n",
    "        \"q\":role,\n",
    "        \"location\":location,\n",
    "        \"api_key\":API_KEY\n",
    "    }\n",
    "    res = requests.get(\"https://serpapi.com/search.json\", params=params)\n",
    "\n",
    "    jobs = res.json().get(\"jobs_results\", [])\n",
    "\n",
    "    for job in jobs:\n",
    "        job[\"search_role\"] = role\n",
    "    all_jobs.extend(jobs)\n",
    "\n",
    "#bronze_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(job) for job in all_jobs]))\n",
    "\n",
    "bronze_df = spark.createDataFrame(all_jobs)\n",
    "bronze_df.write.format(\"delta\").option(\"mergeSchema\",'true').mode(\"overwrite\").saveAsTable(\"bronze_jobs_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8834dcac-ce47-4e1f-8abf-9dacaf030532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(bronze_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce37650b-cd3e-404c-94eb-924f9da735f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Silver Layer** - cleaning and structuring data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cecc300-d720-4ef9-a833-763b0b50c7dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark=SparkSession.builder.appName(\"JobETL_Silver\").getOrCreate()\n",
    "\n",
    "#reading bronze layer\n",
    "df =spark.read.table(\"bronze_jobs_raw\")\n",
    "\n",
    "#cleaning and structuring\n",
    "silver_df= df.selectExpr(\n",
    "    \"title\",\n",
    "    \"company_name\",\n",
    "    \"location\",\n",
    "    \"description\",\n",
    "    \"job_id\",\n",
    "    \"detected_extensions.posted_at as posted_at\",\n",
    "    \"search_role\"\n",
    ").dropna(subset=[\"title\",\"company_name\",\"location\"])\n",
    " \n",
    "display(silver_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f449921-b91e-4c0a-b394-14548417e439",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753102629241}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#trimming company name column and make it upper \n",
    "silver_df=silver_df.withColumn(\"company_name\",trim(upper(col(\"company_name\"))))\n",
    "#dropping na in posted at column\n",
    "silver_df=silver_df.dropna(subset=[\"posted_at\"])\n",
    "\n",
    "#writing the data to silver table\n",
    "silver_df.write.format(\"delta\").option(\"mergeSchema\",'true').mode(\"overwrite\").saveAsTable(\"silver_jobs_raw\")\n",
    "display(silver_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de02c7bb-c61e-4ac7-869a-7bc2c4f0eb1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PROJECT",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
